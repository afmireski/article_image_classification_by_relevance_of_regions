# Importa libs √∫teis para avalia√ß√£o dos modelos
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

from mytypes import ExperimentMetrics, ModelMetrics, StandardExperimentMetrics

def show_predict_infos(y, predict, title="", cmap="Blues", show_plots=True):
    """
    Calcula e exibe m√©tricas de avalia√ß√£o de modelos de classifica√ß√£o.
    
    Args:
        y: Labels verdadeiros
        predict: Predi√ß√µes do modelo
        title: T√≠tulo para os gr√°ficos
        cmap: Mapa de cores para a matriz de confus√£o
        show_plots: Se True, exibe matriz de confus√£o. Se False, apenas calcula m√©tricas
        
    Returns:
        tuple: (accuracy, f1, recall, precision)
    """
    # Mostra a matriz de confus√£o apenas se solicitado
    if show_plots:
        show_confusion_matrix(y, predict, title, cmap)

    # Calcula e mostra as m√©tricas de avalia√ß√£o
    # A acur√°cia √© a propor√ß√£o de predi√ß√µes corretas sobre o total de predi√ß√µes
    accuracy = accuracy_score(y, predict)
    accuracy_percent = accuracy * 100
    
    if show_plots:
        print(f"A acur√°cia no conjunto de testes: {accuracy_percent:.2f}%")

    # O recall √© a propor√ß√£o de predi√ß√µes corretas sobre o total de inst√¢ncias de uma classe
    recall = recall_score(y, predict, average="macro")
    recall_percent = recall * 100
    
    if show_plots:
        print(f"A recall no conjunto de testes: {recall_percent:.2f}%")

    # A precis√£o √© a propor√ß√£o de predi√ß√µes corretas sobre o total de predi√ß√µes de uma classe
    precision = precision_score(y, predict, average="macro")
    precision_percent = precision * 100
    
    if show_plots:
        print(f"A precision no conjunto de testes: {precision_percent:.2f}%")

    # A F1 √© a m√©dia harm√¥nica entre a precis√£o e o recall
    f1 = f1_score(y, predict, average="macro")
    f1_percent = f1 * 100
    
    if show_plots:
        print(f"A F1 no conjunto de testes: {f1_percent:.2f}%")

        # Mostra um relat√≥rio com as m√©tricas de classifica√ß√£o por classe e as m√©tricas calculadas sobre o conjunto todo.
        print("\nRelat√≥rio de Classifica√ß√£o")
        print(classification_report(y, predict))

    return accuracy, f1, recall, precision
    
def show_confusion_matrix(y, predict, title="", cmap="Blues", verbose=False, save_dir='results/confusion_matrixs'):
    import os
    
    # Cria a matriz de confus√£o
    disp = ConfusionMatrixDisplay.from_predictions(y, predict, colorbar=False, cmap=cmap)
    fig = getattr(disp, "figure_", None)
    
    # Adiciona t√≠tulo se fornecido
    if len(title) > 0:
        plt.title(f"Matriz de Confus√£o {title}")
    plt.xlabel("R√≥tulo Previsto")
    plt.ylabel("R√≥tulo Real")
    
    # Salva a matriz de confus√£o se um t√≠tulo foi fornecido
    if len(title) > 0:
        # Cria o diret√≥rio se n√£o existir
        os.makedirs(save_dir, exist_ok=True)
        
        # Gera nome do arquivo baseado no t√≠tulo
        filename = title.lower().replace(" ", "_").replace("-", "_").replace("+", "_")
        filename = "".join(c for c in filename if c.isalnum() or c == "_")  # Remove caracteres especiais
        filepath = os.path.join(save_dir, f"{filename}_confusion_matrix.png")
        
        # Salva a figura
        plt.savefig(filepath, dpi=300, bbox_inches='tight', pad_inches=0.1)
        if verbose:
            print(f"üíæ Matriz de confus√£o salva em: {filepath}")
    
    # Mostra a matriz apenas se verbose for True
    if verbose:
        plt.show()

    # Fecha a figura para evitar acumular muitas figuras abertas na mem√≥ria
    if fig is not None:
        plt.close(fig)
    else:
        plt.close()

def show_relevance_experiment_metrics(metrics: ExperimentMetrics, title=""):
    """
    Exibe m√©tricas de avalia√ß√£o de modelos de classifica√ß√£o.
    
    Args:
        metrics: Tupla com as m√©tricas ((accuracy, f1, recall, precision), especialistas_train_metrics)
        title: T√≠tulo para exibi√ß√£o
    """
    (accuracy, f1, recall, precision), specialists_train_metrics = metrics

    def print_folds(folds):
        return ", ".join([f"{fold*100:.4f}%" for fold in folds])

    print("#" * 40)    
    print(f"M√©tricas Finais Relev√¢ncia {title}:")
    print(f"   üìä Acur√°cia: {accuracy*100:.4f}%")
    print(f"   üìä F1: {f1*100:.4f}%")
    print(f"   üìä Recall: {recall*100:.4f}%")
    print(f"   üìä Precision: {precision*100:.4f}%")
    print("-" * 40)
    print("M√©tricas de Treinamento dos Especialistas:")
    for idx, train_metrics in enumerate(specialists_train_metrics):
        sp_accuracy = train_metrics['accuracy']
        sp_f1 = train_metrics['f1']
        sp_recall = train_metrics['recall']
        sp_precision = train_metrics['precision']

        print(f"   Especialista classe {idx}:")
        print(f"      1Ô∏è‚É£ Acur√°cia M√©dia: {sp_accuracy['mean']*100:.4f}% +- {sp_accuracy['std']*100:.4f}%")
        print(f'        | Folds: {print_folds(sp_accuracy["folds"])}')
        print(f"      2Ô∏è‚É£ F1 M√©dia: {sp_f1['mean']*100:.4f}% +- {sp_f1['std']*100:.4f}%")
        print(f'        | Folds: {print_folds(sp_f1["folds"])}')
        print(f"      3Ô∏è‚É£ Recall M√©dio: {sp_recall['mean']*100:.4f}% +- {sp_recall['std']*100:.4f}%")
        print(f'        | Folds: {print_folds(sp_recall["folds"])}')
        print(f"      4Ô∏è‚É£ Precision M√©dia: {sp_precision['mean']*100:.4f}% +- {sp_precision['std']*100:.4f}%")
        print(f'        | Folds: {print_folds(sp_precision["folds"])}')
    print("#" * 40)
    1
def show_sum_experiment_metrics(metrics: ModelMetrics, title=""):
    """
    Exibe m√©tricas de avalia√ß√£o de modelos de classifica√ß√£o.
    
    Args:
        metrics: Tupla com as m√©tricas (accuracy, f1, recall, precision)
        title: T√≠tulo para exibi√ß√£o
    """
    accuracy, f1, recall, precision = metrics

    print("#" * 40)    
    print(f"M√©tricas Finais Soma {title}:")
    print(f"   üìä Acur√°cia: {accuracy*100:.4f}%")
    print(f"   üìä F1: {f1*100:.4f}%")
    print(f"   üìä Recall: {recall*100:.4f}%")
    print(f"   üìä Precision: {precision*100:.4f}%")
    print("-" * 40)
    print("As m√©tricas dos especialistas s√£o as mesmas da t√©cnica de relev√¢ncia.")
    print("#" * 40)

def show_standard_experiment_metrics(metrics: StandardExperimentMetrics, title=""):
    """
    Exibe m√©tricas de avalia√ß√£o de modelos de classifica√ß√£o.
    
    Args:
        metrics: Tupla com as m√©tricas ((accuracy, f1, recall, precision), especialistas_train_metrics)
        title: T√≠tulo para exibi√ß√£o
    """
    (accuracy, f1, recall, precision), train_metrics = metrics

    def print_folds(folds):
        return ", ".join([f"{fold*100:.4f}%" for fold in folds])

    print("#" * 40)    
    print(f"M√©tricas Finais {title}:")
    print(f"   üìä Acur√°cia: {accuracy*100:.4f}%")
    print(f"   üìä F1: {f1*100:.4f}%")
    print(f"   üìä Recall: {recall*100:.4f}%")
    print(f"   üìä Precision: {precision*100:.4f}%")
    print("-" * 40)
    print("M√©tricas do Treinamento:")

    print(train_metrics)
    
    train_accuracy = train_metrics['accuracy']
    train_f1 = train_metrics['f1']
    train_recall = train_metrics['recall']
    train_precision = train_metrics['precision']

    print(f"      1Ô∏è‚É£ Acur√°cia M√©dia: {train_accuracy['mean']*100:.4f}% +- {train_accuracy['std']*100:.4f}%")
    print(f'        | Folds: {print_folds(train_accuracy["folds"])}')
    print(f"      2Ô∏è‚É£ F1 M√©dia: {train_f1['mean']*100:.4f}% +- {train_f1['std']*100:.4f}%")
    print(f'        | Folds: {print_folds(train_f1["folds"])}')
    print(f"      3Ô∏è‚É£ Recall M√©dio: {train_recall['mean']*100:.4f}% +- {train_recall['std']*100:.4f}%")
    print(f'        | Folds: {print_folds(train_recall["folds"])}')
    print(f"      4Ô∏è‚É£ Precision M√©dia: {train_precision['mean']*100:.4f}% +- {train_precision['std']*100:.4f}%")
    print(f'        | Folds: {print_folds(train_precision["folds"])}')
        
    print("#" * 40)
    

def show_metrics(metrics: ModelMetrics, title=""):
    """
    Exibe m√©tricas de avalia√ß√£o de modelos de classifica√ß√£o.
    
    Args:
        metrics: Tupla com as m√©tricas (accuracy, f1, recall, precision))
        title: T√≠tulo para exibi√ß√£o
    """
    accuracy, f1, recall, precision = metrics

    print("-" * 40)    
    print(f"M√©tricas {title}:")
    print(f"   üìä Acur√°cia: {accuracy*100:.4f}%")
    print(f"   üìä F1: {f1*100:.4f}%")
    print(f"   üìä Recall: {recall*100:.4f}%")
    print(f"   üìä Precision: {precision*100:.4f}%")
    
    print("-" * 40)